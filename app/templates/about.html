<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="../static/css/bluestyle.css" />
<title>The sarcasm detector</title>
</head>


<body>
    <div id="page">
        <div class="topNaviagationLink"><a href="{{url_for('main')}}">Home</a></div>
        <div class="topNaviagationLink"><a href="{{url_for('about')}}">About</a></div>
        <div class="topNaviagationLink"><a href="{{url_for('contact')}}">Contact</a></div>
	</div>
    <div id="mainPicture">
    	<div class="picture">
        	<div id="headerTitle">The sarcasm detector</div>
            <div id="headerSubtext">Learning sarcasm from tweets!</div>
        </div>
    </div>
        <div class="contentBox">
    	<div class="innerBox">
        	<div class="contentTitle"> Why a sarcasm detector?</div>

<br />
          <div class="contentText"><p> This all started when I was looking for a toy project to teach myself <a href="http://en.wikipedia.org/wiki/Natural_language_processing" target="_top">natural language processing</a> (NLP), a field that takes ideas from machine learning and applies them to text data.  Your email spam filter is an application of NLP; there is a learning algorithm that learns how to differentiate a spam email from a regular email by looking at the text content of the email. It had just came out in the <a href="http://www.bbc.com/news/technology-27711109" target="_top">news</a> that the U.S. secret service was looking for a sarcasm detector to improve their intelligence coming from Twitter and I was curious to see whether this was even possible.  This was a perfect project to use NLP, so I decided to give it a try!
</p><br />

        <p> It wasn't clear to me that this was possible because sarcasm is a complicated concept.  Let's go back to the spam filter example for a minute.  If you look at a spam filter algorithm, the features which will be most relevant to the classification of emails will be certain key words: <i>Notspam</i>, <i>Free access</i> or <i>Enlarge your ...</i> for instance.  A good learning algorithm will learn the vocabulary associated with spam emails, so when presented with an email which contains words in that vocabulary the classifier will classify that email as spam.  My initial intuition was that sarcasm detection is more complicated than spam detection, because I didn't think there was a vocabulary associated with sarcastic sentences.  I thought sarcasm is hidden in the tone and the ambivalence of the sentence.  <a href="http://www.merriam-webster.com/dictionary/sarcasm" target="_top">Merriam-Webster</a> defines sarcasm as <i>the use of words that mean the opposite of what you really want to say especially in order to insult someone, to show irritation, or to be funny</i>.  So to detect sarcasm properly a computer would have to figure out that you meant the opposite of what you just said.  It is sometimes hard for Humans to detect sarcasm, and Humans have a much better grasp at the English language than computers do, so this was not going to be an easy task. 
</p>

<div class="contentTitle">   Getting the data</div>
<br />
<p>To train an algorithm to detect sarcasm, we first need some data to train our algorithm on.  Classification is a supervised learning exercise, which means we need to have some sentences labeled as sarcastic and sentences labeled as non-sarcastic so that our classifier can learn the difference between the two.  One option would be to go over an online corpus which might contain some sarcastic sentences, for example online reviews or comments, and label ourselves the sentences by hand.  This can be a very tedious exercise if we want to have a large data set.  The other option is to rely on the people writing the sentences to tell us whether their sentences are sarcastic or not.  This is what we are going to do.  The idea here is to use the <a href="https://dev.twitter.com/" target="_top">Twitter API</a> to stream tweets with the #sarcasm, these will be our sarcastic texts, and other tweets which don't have the #sarcasm, these will be our non-sarcastic texts.   </p>
<br/>

<p> The obvious advantage from taking our data from Twitter is that we can have as much samples as we want.  Every day people write new sarcastic tweets, we can simply stream them and store them in a database.  I ended up collecting 20 000 clean sarcastic tweets and 100 000 clean non-sarcastic tweets over a period of three weeks in June-July 2014 (see section below to understand what a clean tweet is).  Since tweets are often about what is currently happening in the world, it is important to collect the positive (sarcastic) and negative (non-sarcastic) samples during the same time period in order to isolate the sarcasm variable.  </p>
<br/>

<p> However there is a drawback from taking our data from Twitter; it's noisy.  Some people use the #sarcasm to point out that their tweet was meant to be sarcastic, but a Human would not have been able to guess that the tweet was sarcastic without the #sarcasm (example: <i>What a great summer vacation I've been having so far :) #sarcasm</i>).  One may argue however that this is not really noise since the tweet is still sarcastic, at least according to the tweet's owner, and that sarcasm is in the eyes of the beholder.  The converse also happens, someone may write a tweet which is clearly sarcastic but without the #sarcasm.  There are also instances of sarcastic tweets where the sarcasm is in a linked picture or article.  Sometimes tweets are responses to other tweets, in which case the sarcasm can only be understood within the context of the previous tweets.  Sometime the #sarcasm is meant to indicate that, while the tweet itself is not sarcastic, some of its hashtags are (example: <i>Time to do my homework #yay #sarcasm</i>).  I will discuss in the next section how to remove most of that noise, but short of reading all the tweets and labeling them by hand we cannot remove all the noise.   </p>


<div class="contentTitle">  Preprocessing the data</div>
<br />

<p> Before extracting features from our text data it is important to clean it up.  To remove the possibility of having sarcastic tweets in which the sarcasm is either in an attached link or in response to another tweet, we simply discard all tweets which have http addresses in them and all tweets which start with the @ symbol. Ideally we would only collect tweets which are written in English.  When we collect sarcastic tweets, the requirement that it contains #sarcasm makes it very likely that the tweet will be in English.  To maximize the number of English tweets when we collect non-sarcastic tweets, we require that the location of the tweet is either San-Francisco or New-York.  In addition to these steps, we remove any tweets which contain Non-ASCII characters. We then remove all the hashtags, all the friend tags and all mentions of the word sarcasm or sarcastic from the remaining tweets.  If after this pruning stage the tweet is at least 3 words long, we add it to our dataset.  We add this last requirement in order to remove some noise in the sarcastic dataset since I do not believe one can be sarcastic with only 2 words.  Finally, we remove duplicates.  
 </p>

 <div class="contentTitle">  Extracting features</div>
<br />
<p> This is really the meat of the algorithm.  The question here is, what are the variables in a tweet that makes it sarcastic or non-sarcastic? And how do you extract them from the tweet?  To this end I engineered several features that might help the classification of tweets and I tested them on a <a href="http://en.wikipedia.org/wiki/Cross-validation_(statistics)" target="_top">cross-validation</a> set (I will discuss metrics for cross-validation and results in a later section).  The most important features which came out of this analysis are the following:</p>
<br/>
<p> <b>n-grams</b>: More precisely, unigrams and bigrams.  These are just collections of one word (example: <i>really</i>, <i>great</i>, <i>awesome</i>, etc.) and two words (example: <i>really great</i>, <i>super awesome</i>, <i>very weird</i>, etc.).  To extract those, each tweet was <a href="http://en.wikipedia.org/wiki/Tokenization" target="_top">tokenized</a>, <a href="http://en.wikipedia.org/wiki/Stemming" target="_top">stemmed</a>, uncapitalized and then each gram was added to a binary feature dictionary.  </p>
<br/>
<p> <b>Sentiments</b>:  My hypothesis here was that sarcastic tweets might be more negative than non-sarcastic tweets or the other way around.  Moreover, there is often a big contrast of sentiments in sarcastic tweets.  What I mean by this is that tweets often start with a very positive sentiment and end with a very negative sentiment (example: <i>I love being cheated on #sarcasm</i>).  <a href="http://en.wikipedia.org/wiki/Sentiment_analysis" target="_top">Sentiment analysis</a> of tweets is an entire subject on its own so the idea here is to have something simple that can verify my hypothesis.  I first split each tweet in one, two and three parts, and did a sentiment analysis on all parts of the three splitting.  I used two distinct sentiment analyzer.  The first one was my own quick and dirty implementation which uses the <a href="http://sentiwordnet.isti.cnr.it/" target="_top">SentiWordNet</a> dictionary.  This dictionary give a positive sentiment score and a negative sentiment score to each word of the English language.  By looking up words in this dictionary, we can give a sentiment score to each part of the tweets.  The other implementation of the sentiment analysis used the python library <a href="http://textblob.readthedocs.org/en/dev/" target="_top">TextBlob</a> which has a built-in sentiment score function.  </p>
<br/>
<p> <b>Topics</b>: There are words which are often grouped together in the same tweets (example: <i>saturday</i>, <i>party</i>, <i>night</i>, <i>friends</i>, etc.).  We call these various groups of words topics.  The idea here is that if we first learn the topics, then the classifier will just have to learn which topics are more associated with sarcasm and that will make the supervised learning easier and more accurate.  To learn the topics, I used the python library <a href="http://radimrehurek.com/gensim/" target="_top">gensim</a> which implements topic modeling using <a href="http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation" target="_top">latent Dirichlet allocation</a> (LDA).  We first feed all the tweets to the topic modeler which learns the topics.  Then each tweet can be decomposed as a sum of topics, which we use as features.    </p>


<div class="contentTitle">  Choosing a classifier</div>
<br />
<p>There is a very wide range of machine learning algorithms to choose from, most of which are available in the python library <a href="http://scikit-learn.org/stable/" target="_top">Scikit-learn</a>.  However, most of the implementations of these algorithms do not accept sparse matrices as inputs, and since we have a large number of nominal features coming from our n-grams features it is imperative that we encode our features in a sparse matrix.  I ended up trying naive Bayes, logistic regression and <a href="http://en.wikipedia.org/wiki/Support_vector_machine" target="_top">support vector machine</a> (SVM) with a linear kernel.  I got the best results in cross validation using SVM, with an euclidean regularization coefficient of 0.1.  
</p>

<br/>

<p> The metric I use to guide my cross-validation is the <a href="http://en.wikipedia.org/wiki/Precision_and_recall" target="_top">F-score</a>.  This is a good metric when we have a lot more samples from a category than the other.  In our case we have 5 times more non-sarcastic tweets than sarcastic tweets.  If we just use the accuracy as a metric, that is the number of correct predictions divided by the total number of tweets in our cross-validation, then a simple classifier which always predicts the tweets as non-sarcastic would get a 83% accuracy. This is obviously very misleading so we need a better metric.  We can do better by considering precision and recall for the sarcastic category.  Precision is the number sarcastic tweets correctly identified divided by the total number of tweets classified as sarcastic, while recall is the number sarcastic tweets correctly identified divided by the total number of sarcastic tweets in the cross validation set.  Both precision and recall would be equal to 0% with a dumb classifier which always predicts tweets to be non-sarcastic, so these are already much better score to quantify the quality of a sarcasm classifier. The F-score is simply the harmonic mean of precision and recall.

<div class="contentTitle">  Results and insights</div>
<br />


<p>
To understand the relative importance of each set of features, we can train our algorithm using only n-grams, only the sentiments or only the topics and look at the corresponding F-score. For the n-grams we get F = 0.56, while for the sentiments we get F = 0.41 and for the topics we get F = 0.35. Note that the addition of all the features will not give an F-score which is a linear addition of the individual F-scores.  Initially I thought that the main features would come from the sentiment analysis but the n-grams are by far the most important features.  This shows that there is a vocabulary associated with sarcasm after all, and it's not just about the tone!
</p>
<br/>

<p>
To gain some insight into what the algorithm has learned, we can look at the coefficients of our features in the trained SVM to see which ones are the most important.  For n-grams, the features which are the most important to classify sarcastic tweets are <i>just what</i>, <i>yay</i>, <i>perfect time</i>, <i>a blast</i>, <i>shocker</i>, <i>just love</i>, etc.  These n-grams all make perfect sense, but since I streamed the tweets during the 2014 FIFA world cup I also ended up with tree relevant unigrams for sarcastic tweets that are somewhat unexpected: <i>rooney</i>, <i>brazil</i> and <i>suarez</i>.  The most relevant n-grams for the classification of non-sarcastic tweets are <i>feel good</i>, <i>spend some</i>, <i>pray</i>, <i>too funny</i>, <i>be nice</i>, <i>goodnight</i>, etc. For the sentiment analysis, the classifier learned that sarcastic tweets are overall slightly more negative than non-sarcastic tweets and that the first third of a sarcastic tweet is often positive while the last third of a tweet is more often negative.  However what seems to matter the most is the subjectivity which is also measured by the library Textblob.  This means that sarcastic tweets are more about expressing feelings, either positive or negative, than non-sarcastic tweets.  Finally, the topic modeler learned that topics which contain words such as <i>love</i>, <i>life</i>, <i>today</i>, <i>lol</i>, <i>feel</i>, <i>bad</i> etc. are more associated with sarcastic tweets while topics which contain words such as <i>jersey</i>, <i>procrastinating</i>, <i>hot</i>, <i>complain</i>, <i>storm</i>, <i>mom</i>, etc. are more associated with non-sarcastic tweets.  
</p>
<br/>



<p>
After the cross-validation phase, I tested my sarcasm classifier on a separate test set made of 24 000 tweets, and obtained an <b>F-score of 0.60</b>.  This is an improvement over the previous studies in sarcasm detection using tweets which obtained F-scores in the range 0.50-0.55, see references [1]-[4].  The previous work on sarcasm detection uses only n-grams features and/or rule-based features and classification.  Our analysis improves over those previous ones by incorporating a sentiment decomposition analysis and a topic modeling analysis.  Moreover we used a modern machine learning algorithm, SVM, which is not rule-based.
</p>


<div class="contentTitle">  Outlook
</div>
<br />
<p> I think sarcasm detection is a really fascinating subject, and I'm not being sarcastic!  It's not clear to me whether or not my sarcasm detector would be very useful for the U.S. secret service, but I do feel confident that I answered my original question; it is possible to detect sarcasm using tools from NLP.  One quick way to improve my detector would be to incorporate a spell check for the tweets.  This should reduce the number of dimensions of the dictionary for the n-grams features and improve the sentiment analysis.  I did some preliminary experiments with the spell check of the library Textblob but I did not get any improvements using it, perhaps a better spell checker would do better.  
 </p>


<div class="contentTitle">  The tools used</div>
<br />
Here is a laundry list of tools used in this project:
<p> <b>Back end</b>: Python with Numpy, Scipy, Scikit-learn, NLTK, gensim, Textblob and tweepy.  </p>
<p> <b>Front end</b>: Python with Flask, HTML, CSS, Javascript and the whole thing hosted by <a href="https://www.pythonanywhere.com/" target="_top">pythonanywhere.com</a>.   </p>
<br/>
<p> 
The code can be found on <a href="https://github.com/MathieuCliche/Sarcasm_detector" target="_top">https://github.com/MathieuCliche/Sarcasm_detector</a>
 </p>


<div class="contentTitle">  References
</div>
<br />
[1] Dmitry Davidov, Oren Tsur and Ari Rappoport, <a href="http://dl.acm.org/citation.cfm?id=1870568.1870582
" target="_top"><i>Semi-Supervised Recognition of Sarcastic Sentences
in Twitter and Amazon</i></a>
<!-- pattern matching, F=0.545 --><br/>
[2] Ellen Riloff, Ashequl Qadir, Prafulla Surve, Lalindra De Silva, Nathan Gilbert and Ruihong Huang, <a href="http://www.cs.utah.edu/~huangrh/official-sarcasm-cameraReady-v2.pdf" target="_top"><i>Sarcasm as Contrast between a Positive Sentiment and Negative Situation</i></a>
<!-- positive sentiment negative situation, F=0.51--><br/>
[3] Roberto Gonzalez-Ibanez, Smaranda Muresan and Nina Wacholder,<a href="http://dl.acm.org/citation.cfm?id=2002850
" target="_top"><i> Identifying Sarcasm in Twitter: A Closer Look</i></a>
<!-- unigrams and emoticons 70% accuracy --><br/>
[4] Christine Liebrecht, Florian Kunneman  and Antal Van den Bosch, <a href="http://www.aclweb.org/anthology/W13-1605" target="_top"><i>The perfect solution for detecting sarcasm in tweets #not</i></a>
<!-- Deutch paper--> <br/>



        </div>
    </div>

        </body>
</html>
