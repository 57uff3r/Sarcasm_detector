<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="../static/css/anoceanofsky.css" />
<title>The sarcasm detector</title>
</head>


<body>
    <div id="page">
        <div class="topNaviagationLink"><a href="{{url_for('main')}}">Home</a></div>
        <div class="topNaviagationLink"><a href="{{url_for('about')}}">About</a></div>
        <div class="topNaviagationLink"><a href="{{url_for('contact')}}">Contact</a></div>
	</div>
    <div id="mainPicture">
    	<div class="picture">
        	<div id="headerTitle">The sarcasm detector</div>
            <div id="headerSubtext">Learning sarcasm from tweets!</div>
        </div>
    </div>
        <div class="contentBox">
    	<div class="innerBox">
        	<div class="contentTitle"> Why a sarcasm detector ?</div>

<br />
          <div class="contentText"><p> This all started when I was looking for a toy project to teach myself natural language processing (NLP), a field that takes ideas from machine learning and applies them to text data.  Your email spam filter is an application of NLP; there is a learning algorithm that learns how to differentiate a spam email from a regular email by looking at the text content of the email. It had just came out in the news that the U.S. secret service were looking for a sarcasm detector to improve their intelligence coming from Twitter, and I was curious to see whether this was even possible.  This was a perfect project to use NLP, so I decided to give it a try!
</p><br />

        <p> It wasn't clear to me that this was possible because sarcasm is a complicated concept.  Let's go back to the spam filter example for a minute.  If you look at a spam filter algorithm, the features which will be most relevant to the classification of emails will be certain key words; "Notspam", "Free acess" or "Enlarge your ..." for instance.  A good learning algorithm will learn that these words are more often used in spam emails, so when presented with an email which contains these words the classifiers will classify that email as spam.  Sarcasm is more complicated, because while there maybe a few keywords which are more associated with sarcastic sentences than non-sarcastic sentences, sarcasm is often hidden in the tone and the ambivalence of the sentence itself.  Merriam-webster defines sarcasm as "the use of words that mean the opposite of what you really want to say especially in order to insult someone, to show irritation, or to be funny".  So to detect sarcasm properly a computer would have to figure out that you meant the opposite of what you just said.  It is sometimes hard for Humans to detect sarcasm, and Humans have a much better graps at the English language than computers do, so this was not going to be an easy task. 
</p>

<div class="contentTitle">   Getting the data</div>
<br />
<p>To train an algorithm to detect sarcasm, you first need some data to train your algorithm on.  Classification is a supervised learning excercice, which means we need to have some sentences labeled as sarcastic and sentences labeled as non-sarcastic so that our classifier can learn the difference between the two.  One option would be to go over an online corpus which might contain some sarcastic sentences, for exemple online reivews or comments, and label ourself the sentences by hand.  This can be a very tedious exercice if we want to have a large data set.  The other option is to rely on the people writing the sentences to tell us whether this was a sarcastic sentence or not.  This is what we are going to do.  The idea here is to use the Twitter API to stream tweets with the #sarcasm, these will be our sarcastic texts, and other tweets which don't have the #sarcasm, these will be our non-sarcastic texts.   </p>
<br/>

<p> The obvious advantage from taking our data from Twitter is that we can have as much samples as we want.  Every day people write more and more sarcastic tweets, we can simply stream them and store them in a database.  I ended up collecting 20 000 clean sarcastic tweets and 100 000 clean non-sarcastic tweets over a period of three weeks in June-July 2014 (see section below to understand what a clean tweet is).  Since tweets are often about what is currently happening in the world, it is important to collect the positive (sarcastic) and negative (non-sarcastic) sample during the same time period in order to isolate the sarcasm variable.  </p>
<br/>

<p> However there is a drawback from taking our data from Twitter, it's noisy.  Some people use the #sarcasm to point out that their tweet was meant to be sarcastic, but a Human would not have been able to guess that the tweet was sarcastic without the #sarcasm (exemple: "What a great summer vacation I've been having so far :) #sarcasm").  One may argue however that this is not really noise since the tweet is still sarcastic, at least according to the tweet's owner, and that sarcasm is in the eyes of the beholder.  The converse also happens, someone may write a tweet which is clearly sarcastic but without the #sarcasm.  There are also instances of sarcastic tweets where the sarcasm is in a linked picture or article.  Sometimes tweets are responses to other tweets, in which case the sarcasm can only be understood within the context of the previous tweets.  Sometime the #sarcasm is meant to indicate that, while the tweet is not sarcastic, some of its hashtags are (example: "Time to do my homework #yay #sarcasm").  I will discuss in the next section how to remove most of that noise, but short of reading all the tweets and labeling them by hand we cannot remove all the noise.   </p>


<div class="contentTitle">  Preprocessing the data</div>
<br />

<p> Before extracting features from our text data it is important to clean it up a bit.  To remove the possibility of having sarcastic tweets in which the sarcasm is either in an attached link or in response to another tweet, we simply discard all tweets which have http addresses in them and all tweets which start with the @ symbol. When we collect sarcastic tweets, the requirement that it contains #sarcasm makes it very likely that the tweet will be in English.  To maximise the number of English tweets when we collect non-sarcastic tweets, we require that the location of the tweet is either San-Francisco or New-York.  In addition to these steps, we remove any tweets which contain Non-ASCII characters. We then remove all the hashtags, all friend tags and all mentions of the word sarcasm or sarcastic from the remaining tweets.  If after this pruning stage the tweet is at least 3 words long, we add it to our dataset.  We add this requirement in order to remove some noise in the sarcastic dataset since I do not beleive one can be sarcastic with only 2 words.  Finally, we remove duplicates.  
 </p>

 <div class="contentTitle">  Extracting features</div>
<br />
<p> This is really the meat of the algorithm.  The question here is, what are the variables in a tweet that make it sarcastic or non-sarcastic? And how do you extract them from the tweet?  To this end I engineered several features that I think might help the classification of tweets and tested them on a cross validation set (I will discuss metrics and results in a later section).  The most important features which came out of this analysis are the following:</p>
<br/>
<p> <b>n-grams</b>: More precisely, unigrams and bigrams.  These are just collections of one word (example: "really", "great", "awesome", etc.) and two words (example: "really great", "super awesome", "very weird", etc.).  To extract those, each tweet was tokenized, stemmed, uncapitalized and then each grams was added to a binary feature dictionnary.  </p>
<br/>
<p> <b>Sentiments</b>:  My hypothesis here was that sarcastic tweets might be more negative than non-sarcastic tweets or the other way around.  Moreover, there is often a big constrast of sentiments in sarcastic tweets.  Indeed, sarcastic tweets often start with a very positive sentiment and end with a very negative sentiment (example: "I love being cheated on #sarcasm").  Sentiment analysis of tweets is an entire subject on its own so the idea here is to have something simple that explores my hypothesis.  I first splitted each tweet in one, two and three parts, and did a sentiment analysis on all parts of the three splitting.  I used two distinct sentiment analyzer.  The first one was my own quick and dirty implementation which uses the "SentiWordNet" dictionnary.  This dictionnary give a positive sentiment score and a negative sentiment score to each word of the English languge.  By looking up words in this dictionnary, we can give a sentiment score to each part of the tweets.  The other implementation of the sentiment analysis used the python librairy TextBlob which has a built-in sentiment score function.  </p>
<br/>
<p> <b>Topics</b>: There are words which are often grouped together in the same tweets (example: "saturday", "party", "night", "friends", etc.).  We call these various groups of words topics.  The idea here is that if we first learn the topics, then the classifier will just have to learn which topics are more associated with sarcasm and that will make the supervised learning easier and more accurate.  To learn the topics, I used the python librairy gensim which implements topic modeling using latent Dirichlet allocation (LDA).  We first feed all the tweets to the topic modeler which learns the topics.  Then each tweet can be decomposed as a sum of topics, which we use as features.    </p>


<div class="contentTitle">  Choosing a classifier</div>
<br />
<p>There is a very wide range of possible algorithms to choose from, most of which are available in the python librairy Scikit-learn.  However most of them do not accept sparce matrices as inputs, and since we have a large number of nominal features coming from our ngrams it is imperative that we encode our features in a sparce matrix.  I ended up trying naive Bayes, logistic regression and support vector machine (SVM) with a linear kernel.  I got the best results in cross validation using SVM, with a euclidian regularization coefficient of 0.1.  
</p>

<br/>

<p> The metric I use to guide my cross-validation is the F-score.  This is a good metric when we have a lot more sample from a category than the other.  In our case we have 5 times more non-sarcastic tweets than sarcastic tweets.  If we just use the accuracy as a metric, that is the number of correct predictions divided by the total number of tweets in our cross-validation, then a simple classifier which always predicts the tweets as non-sarcastic would get a 83% accuracy. This is obviously very misleading so we need a better metric.  We can do better by considering precision and recall for the sarcastic category.  Precision is the number sarcastic tweets correctly identified divided by the total number of tweets classified as sarcastic, while recall is the number sarcastic tweets correctly identified divided by the total number of sarcastic tweets in the cross validation set.  Both precision and recall would be equal to 0% with a dumb classifier which always predicts tweets to be non-sarcastic, so these are already much better score to quantify the quality of a sarcasm classifier. The F-score is simply the harmonic mean of precision and recall.

<div class="contentTitle">  Results and insights</div>
<br />


<p>
To understand the relative importance of each sets of feature, we can train our algorithm using only n-grams, only the sentiments or only the topics.  While the addition of all the features will not give a F-score which is a linear addition of the individual F-scores, this gives an idea of the relative importance of each feature set.  While initially I was hoping that the main features would come from the sentiment analysis, you can see that the n-grams are by the most important features.  
</p>
<br/>

<p>
To gain some insights into what the algorithm has learned, we can look at the coefficients of our features to see which ones are the most important.  For n-grams, the features which are the most important to classify sarcastic tweets are "just what", "yay", "perfect time", "a blast", "shocker", "just love", etc.  Since I streamed the tweets during the 2014 soccer world cup, I also ended up with relevant n-grams for sarcastic tweets such as "rooney", "brazil" and "suarez".  The most relevant n-grams for the classification of non-sarcastic tweets are "feel good", "spend some", "pray", "too funny", "be nice", "goodnight", etc. For the sentiment analysis, the classifier learned that sarcastic tweets are overall more negative than non-sarcastic tweets, and that the first third of a sarcastic tweet is often positive while the last third of a tweet is more often negative.  However what seems to matter the most is the subjectivity which is also measured by the librairy Textblob.  This means that sarcastic tweets are more about expressing feelings, either positive or negative, than non-sarcastic tweets.  Finally, the topic modeler learned that topics which contain words such as "love", "life", "day", "much", "feel", "home" etc. are more associated with sarcastic tweets while topics which contain words such as "justice", "procrastinating", "hot", "complain", "storm", "beauty", etc. are more associated with non-sarcastic tweets.  
</p>
<br/>



<p>
After the cross-validation phase, I tested my sarcasm classifier on a seperate test set, and obtained an F-score of 0.60.  This is an improvement over the previous studies in sarcasm detection using tweets which obtained F-scores in the range 0.50-0.55, see [1]-[4].  The previous work on sarcasm detection uses only n-grams features and/or rule-based features and classification.  Our analysis improves over those previous ones by incorporating a sentiment decomposition analysis and a topic modeling analysis.  Moreover we used a modern machine learning algorithm, SVM, which is not rule-based.
</p>


<div class="contentTitle">  Outlook
</div>
<br />
<p> I think sarcasm detection is a really fascinating subject, and I'm not being sarcastic! ;).  It's not clear to me wether or not my sarcasm detector would be very useful for the U.S. secret service, but I do feel confident that I answered my original question; it is possible to detect sarcasm using tools from NLP.  One quick way to improve my detector would be to incoporate a spell check for the tweets.  This should reduce the number of dimensions of the dictionnary for the ngrams features and improve the sentiment analysis.  I experimented with the spell check of the librairy Textblob but I did not get any improvements using it, perhaps a better spell checker would do better.  
 </p>


<div class="contentTitle">  The tools used</div>
<br />
Here is a laundry list of tools used in this project:
<p> <b>Backend</b>: Python with Numpy, Scipy, Scikit-learn, NLTK, gensim, Textblob and tweepy.  </p>
<p> <b>Frontend</b>: Python with Flask, HTML, CSS, Javascript and the whole thing hosted by pythonanywhere.com.   </p>
<br/>
<p> 
The code can be found on https://github.com/MathieuCliche/Sarcasm_detector
 </p>


<div class="contentTitle">  References
</div>
<br />
[1] Dmitry Davidov, Oren Tsur and Ari Rappoport, <i>Semi-Supervised Recognition of Sarcastic Sentences
in Twitter and Amazon</i>, http://dl.acm.org/citation.cfm?id=1870568.1870582
<!-- pattern matching, F=0.545 --><br/>
[2] Ellen Riloff, Ashequl Qadir, Prafulla Surve, Lalindra De Silva, Nathan Gilbert and Ruihong Huang, <i>Sarcasm as Contrast between a Positive Sentiment and Negative Situation</i>, http://www.cs.utah.edu/~huangrh/official-sarcasm-cameraReady-v2.pdf
<!-- positive sentiment negative situation, F=0.51--><br/>
[3] Roberto Gonzalez-Ibanez, Smaranda Muresan and Nina Wacholder, <i> Identifying Sarcasm in Twitter: A Closer Look</i>, http://dl.acm.org/citation.cfm?id=2002850
<!-- unigrams and emoticons 70% accuracy --><br/>
[4] Christine Liebrecht, Florian Kunneman  and Antal Van den Bosch, <i>The perfect solution for detecting sarcasm in tweets #not </i>, http://www.aclweb.org/anthology/W13-1605
<!-- Deutch paper--> <br/>



        </div>
    </div>

        </body>
</html>
